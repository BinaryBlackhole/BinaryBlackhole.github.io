<!DOCTYPE html>
<!-- saved from url=(0050)http://getbootstrap.com/examples/starter-template/ -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="personal page of Sagar Chakraborty">
    <meta name="author" content="Sagar Chakraborty">
    <link rel="icon" href="resources/icons/G.png">
    <title>Sagar Chakraborty</title>

    <!-- Bootstrap core CSS -->
    <link href="./bootstrap/bootstrap.min.css" rel="stylesheet">

    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <link href="./bootstrap/ie10-viewport-bug-workaround.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="./bootstrap/style.css" rel="stylesheet">
	<link href="./bootstrap/css" rel="stylesheet" type="text/css">
	
	
    <!-- Just for debugging purposes. Don't actually copy these 2 lines! -->
    <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->
    <script src="./bootstrap/ie-emulation-modes-warning.js"></script>

	<script>
	  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	
	  ga('create', 'UA-81912424-1', 'auto');
	  ga('send', 'pageview');
	
	</script>
	
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  
  </head>
<body>
	<div class="container">
	<div  class="jumbotron">
		
		<div style="float:left; margin-left:-55px;  margin-right:40px"> <img src="resources/sagar.jpg" height="230"></div>
		<div class="container">
			<h2 class="hh">Sagar Chakraborty</h2>
			<h4> <a href="http://www.alliedmedia.com/docvision/">Visual Intelligence Laboratory</a></h4>
			<h4>Allied Media Inc</h4>
			<h4>Kolkata, West Bengal , India</h4>
			<!--<a href = "https://sites.google.com/site/gurkirtcv/"> Visit my personal page on google.sites</a>-->
      <a href="sagar.chakraborty@alliedmedia.com" target="_blank"><img src="resources/icons/outlook.png" height="32" ></a> 
			<a href="csagar963@gmail.com" target="_blank" ><img src="resources/icons/mail.png" height="32"></a>
			<a href="https://github.com/BinaryBlackhole/" target="_blank" ><img src="resources/icons/github_alt.png" height="32"></a> 
			<a href="https://www.linkedin.com/in/binaryblackhole/" target="_blank" ><img src="resources/icons/linkedin.png" height="32"></a>  
			
			
		</div>
	</div>
	</div>

  <div class="container">
    <h2 class="hh">About Me </h2>
    <p>
      <a href="https://drive.google.com/file/d/1vfKgukAwcRexOY-gr49g9Cc7z8HtvMVh/g">Get my resume here</a>
    </p>
		<p>
      
      I am a AI Product Manager in the Visual Intelligence Lab, Allied Media Inc.
			In VIL I work on Representation Learning, Intelligence Information extraction, Document layout analysis & Information generation & retrieval. 
			I also am working with Dr. Ruhi Sonal in  <a href="https://cmceiitj.wordpress.com/">Centre for Mathematical and Computational Economics</a>
			at Indian Institute of Technology, Jodhpur. In CMCE, my research is mainly focused on Artificial Intelligence in Game Theory and Economics. 
			
		<p>	
			Earlier, I was researcher under <a href="https://sites.google.com/view/mphlin">Dr. Mark P-H Lin</a> in Electronic Design Automation Lab at CCU, Taiwan, Sponsered by TEEP@India scholarship.
			In 2018, I joined Amazon and worked there for 1 year as an Analyst. 
			I received Bachelor of Technology degree in Electronics and Communication Engineering from <a href="https://www.aliah.ac.in/">Aliah University</a>, Kolkata,
			during which I had chance to work under the supervision of <a href="https://aliah.ac.in/department/people-details.php?key=electronics-and-communication-engineering&id=81">Prof. Somsubhra Talapatra</a>.
		</p>
	
	</div>
	
	<div class="container">

    <h2 class="hh">News</h2>
<!--     <p> <span style="color:#DC7633">I will be graduating towards the end of 2019; looking for full-time researcher and Postdoc positions.</span> </p> -->
<!-- <p> Download <a href="resources/test.zip" download>test.zip</a> to evaluate your models on <a href="https://saras-esad.grand-challenge.org/">SARAS-ESAD challenge</a></p> -->
    <p> June 2021: VIL, Allied Media is recruiting Jr. Machine Learning Engineers
    <p> June 2021: I am looking for collaborators to participate in competitions in <a href="https://sites.google.com/view/roadchallangeiccv2021/"> ICCV </a> and <a href="https://sites.google.com/view/sscl-workshop-ijcai-2021/challenges"> IJCAI 2021 </a> 
    
    

	<!--/div>
	
	<!--div class="container">

		<h2 class="hh">Contests</h2>
		<p>Charades Challenge, <a href="http://vuchallenge.org/charades.html">2017</a>: Acton Recognition, Rank: 2/10, Temporal Action Segmentation, Rank: 3/6.</p>
		<p>ActivityNet Challenge, <a href="http://activity-net.org/challenges/2017/evaluation.html">2017</a>: Untrimmed Video Classification, Rank: 3/29.</p> 
		<p>ActivityNet Challenge, <a href="http://activity-net.org/challenges/2016/evaluation.html">2016</a>: Untrimmed Video Classification, Rank: 10/24, <span style="color:blue">Actvity detection</span>, Rank: 2/6.</p>
		<p>ChaLearn Looking at People Challenge, 2014 , Gesture detection, Rank: 7/17.</p>
		<p>ChaLearn Looking at People Challenge, 2013 , Gesture detection, Rank: 17/54.</p>

	</div>
	-->
	<!--div class="container">

	<h2 class="hh" style="margin-bottom:40px">Publications <a href="https://scholar.google.com/citations?user=w8XHUMIAAAAJ&hl=en" target="_blank"><img src="resources/icons/google_scholar.png" ></a> </h2>
  
  <1--div class="pubwrap">
    <div class="row">
      <div class="col-md-6">
        <div class="pubimg">
          <img src="./resources/projects/rcn.svg" scale:2>
        </div>
      </div>
      <div class="col-md-6">
        <div class="pub">
          <div class="pubt">Recurrent Convolutions for Causal 3D CNNs</div>
          <div class="pubd">we propose a novel Recurrent Convolutional Network (RCN), which relies on recurrence to capture the temporal context across frames at each network level. 
            Our network decomposes 3D convolutions into (1) a 2D spatial convolution component, and (2) an additional hidden state
            1 × 1 convolution, applied across time. 
            The hidden state at any time t is assumed to depend on the hidden state at t − 1
            and on the current output of the spatial convolution component. 
            As a result, the proposed network: (i) produces causal outputs, (ii) provides flexible temporal reasoning, (iii) preserves temporal resolution.</div>
          <div class="puba">Gurkirt Singh, Fabio Cuzzolin.</div>
          <div class="pubv"><a href="https://holistic-video-understanding.github.io/workshops/iccv2019.html">HVU - ICCVW 2019 </a></div>
          <div class="publ">
      <ul>
        <li><a href="https://arxiv.org/pdf/1811.07157.pdf">PDF</a></li>
        <!-- <li><a href="#">Poster</li> -->
        <!-- <li><a href="resources/poster_cecvW.pdf">Presentation</li> -->
      </ul>
          </div>
        </div>
      </div>
    </div>
  </div> 


  <!--div class="pubwrap">
    <div class="row">
      <div class="col-md-6">
        <div class="pubimg">
          <img src="./resources/projects/video-captioning.png" scale:2>
        </div>
      </div>
      <div class="col-md-6">
        <div class="pub">
          <div class="pubt">End-to-End Video Captioning</div>
          <div class="pubd">we propose to optimise both encoder and decoder simultaneously in an end-to-end fashion. 
            In a twostage training setting, we first initialise our architecture using pre-trained encoders and decoders – then, 
            the entire network is trained end-to-end in a fine-tuning stage to learn the most relevant features for video caption generation. 
            In our experiments, we use GoogLeNet and Inception-ResNetv2 as encoders and an original Soft-Attention (SA-) LSTM as a decoder. 
            Analogously to gains observed in other computer vision problems, we show that end-to-end training  significantly improves over the traditional, disjoint training
            process</div>
            <div class="puba">Silvio Olivastri, Gurkirt Singh, Fabio Cuzzolin.</div>
          <div class="pubv"><a href="https://holistic-video-understanding.github.io/workshops/iccv2019.html">HVU - ICCVW 2019 </a></div>
          <div class="publ">
      <ul>
        <li><a href="https://arxiv.org/pdf/1904.02628.pdf">PDF</a></li>
        <!-- <li><a href="#">Poster</li> -->
        <!-- <li><a href="resources/poster_cecvW.pdf">Presentation</li> -->
      </ul>
          </div>
        </div>
      </div>
    </div>
  </div>
  -->

  
  <!--div class="pubwrap">
      <div class="row">
        <div class="col-md-6">
          <div class="pubimg">
            <img src="./resources/projects/ahb2018_tpnet.png">
          </div>
        </div>
        <div class="col-md-6">
          <div class="pub">
            <div class="pubt">Predicting Action Tubes</div>
            <div class="pubd">we present a method to predict an entire ‘action tube’ in a trimmed video just by observing a smaller subset of video. We propose a Tube Prediction network (TPnet) which jointly predicts the past, present and future bounding boxes along with
             their action classification scores. At test time TPnet is used in a (temporal) slid-
             ing window setting, and its predictions are put into a tube estimation framework
            to construct/predict the video long action tubes not only for the observed part of
               the video but also for the unobserved part</div>
            <div class="puba">Gurkirt Singh, Suman Saha, Fabio Cuzzolin.</div>
            <div class="pubv">AHB - ECCVW 2018 </div>
            <div class="publ">
        <ul>
          <li><a href="https://arxiv.org/pdf/1808.07712.pdf">PDF</a></li>
          <li><a href="resources/poster_eccvW.pdf">Poster</a></li>
          <li><a href="resources/poster_cecvW.pdf">Presentation</a></li>
        </ul>
            </div>
          </div>
        </div>
      </div>
    </div>

<div class="pubwrap">
      <div class="row">
        <div class="col-md-6">
          <div class="pubimg">
            <img src="./resources/projects/accv_overview.png">
          </div>
        </div>
        <div class="col-md-6">
          <div class="pub">
            <div class="pubt">TraMNet - Transition Matrix Network for Efficient Action Tube Proposals</div>
            <div class="pubd">Current state-of-the-art methods solve spatio-temporal action localisation by extending 2D anchors to 3D-cuboid proposals on stacks of frames, to generate sets of temporally connected bounding boxes called action micro-tubes. To avoid this problem we introduce a Transition-Matrix-based Network (TraMNet) which relies on computing transition probabilities between anchor proposals while maximising their overlap with ground truth bounding boxes across frames, and enforcing sparsity via a transition threshold. As the resulting transition matrix is sparse and stochastic, this reduces the proposal hypothesis search space from O(n^f) to the cardinality of the thresholded matrix. </div>
            <div class="puba">Gurkirt Singh, Suman Saha, Fabio Cuzzolin.</div>
            <div class="pubv">ACCV 2018</div>
            <div class="publ">
        <ul>
          <li><a href="https://arxiv.org/pdf/1808.00297.pdf">PDF</a></li>
          <li><a href="#">Poster</a></li>
          <li><a href="#">Presentation</a></li>
        </ul>
            </div>
          </div>
        </div>
      </div>
    </div>

	<div class="pubwrap">
      <div class="row">
        <div class="col-md-6">
          <div class="pubimg">
            <img src="./resources/projects/cvpr2017.png">
          </div>
        </div>
        <div class="col-md-6">
          <div class="pub">
            <div class="pubt">Online Real-time Multiple Spatiotemporal Action Localisation and Prediction</div>
            <div class="pubd">We present a method for multiple spatiotemporal action localisation, 
            classification, and early prediction based on a single deep learning framework, 
            which able to work in an online and real time contraints.</div>
            <div class="puba">Gurkirt Singh, Suman Saha, Michael Sapienza, Philip Torr, Fabio Cuzzolin.</div>
            <div class="pubv">ICCV 2017 </div>
            <div class="publ">
				<ul>
				  <li><a href="https://arxiv.org/pdf/1611.08563.pdf">PDF</a></li>
				  <li><a href="https://github.com/gurkirt/realtime-action-detection">Code</a></li>
				  <li><a href="resources/ICCV2017_final_poster.pdf">Poster</li>
          <li><a href="https://www.youtube.com/watch?v=e6r_39ETe-g&t=1s">Video</a></li>
				</ul>
            </div>
          </div>
        </div>
      </div>
    </div>
	
	<div class="pubwrap">
      <div class="row">
        <div class="col-md-6">
          <div class="pubimg">
            <img src="./resources/projects/iccv2017_amtnet.png">
          </div>
        </div>
        <div class="col-md-6">
          <div class="pub">
            <div class="pubt">AMTnet: Action-Micro-Tube regression by end-to-end trainable deep architecture.</div>
            <div class="pubd">Dominant approaches provides sub-optimal solutions to the action dection problem, as they rely on seeking frame-level detections and construting tubes from them. In this paper we radically depart from current practice, and take a first step towards the design and implementation of a deep network architecture able to classify and regress video-level micro-tubes.</div>
            <div class="puba">Suman Saha, Gurkirt Singh, Fabio Cuzzolin.</div>
            <div class="pubv">ICCV 2017 </div>
            <div class="publ">
				<ul>
				  <li><a href="https://arxiv.org/pdf/1704.04952.pdf">PDF</a></li>
				  <li><a href="#">Poster-Coming-soon</a></li>
				</ul>
            </div>
          </div>
        </div>
      </div>
    </div>


	<div class="pubwrap">
      <div class="row">
        <div class="col-md-6">
          <div class="pubimg">
            <img src="./resources/projects/BMVC2016icon.png">
          </div>
        </div>
        <div class="col-md-6">
          <div class="pub">
            <div class="pubt">Deep Learning for Detecting Multiple Space-Time Action Tubes in Videos</div>
            <div class="pubd">In this work we propose a new approach to the spatio-temporal
              localisation (detection) and classification of multiple concurrent actions within
              temporally untrimmed videos. We demonstrate the performance of our algorithm on the challenging
              UCF101, J-HMDB-21 and LIRIS-HARL datasets, achieving new state-of-the-art results
              across the board and significantly lower detection latency at test time.</div>
            <div class="puba">Suman Saha, Gurkirt Singh, Michael Sapienza, Philip Torr, Fabio Cuzzolin.</div>
            <div class="pubv">BMVC 2016 </div>
            <div class="publ">
				<ul>
				  <li><a href="http://arxiv.org/pdf/1608.01529v1.pdf">PDF</a></li>
				  <li><a href="http://sahasuman.bitbucket.org/bmvc2016/index.html">Project page</a></li>
				  <li><a href="https://bitbucket.org/sahasuman/bmvc2016_code">Code</a></li>
				  <li><a href="https://www.youtube.com/watch?v=vBZsTgjhWaQ">Demo Video</a></li>
				</ul>
            </div>
          </div>
        </div>
      </div>
    </div>
		
	<div class="pubwrap">
      <div class="row">
        <div class="col-md-6">
          <div class="pubimg">
            <img src="./resources/projects/actnet2016.png">
          </div>
        </div>
        <div class="col-md-6">
          <div class="pub">
            <div class="pubt">Untrimmed Video Classification for Activity Detection: submission to ActivityNet Challenge</div>
            <div class="pubd">In this work we propose a simple, yet effective, method for the temporal detection
						of activities in temporally untrimmed videos with the help of untrimmed classification.
			            This method secured the <span style="color:red">2nd</span> place at ActivityNet
						Challenge 2016 in activity detection task [<a href="http://activity-net.org/challenges/2016/program.html">Results</a>]</div>
            <div class="puba">Gurkirt Singh and Fabio Cuzzolin.</div>
            <div class="pubv">CVPR 2016 ActivityNet workshop, 2nd place in detection task.</div>
            <div class="publ">
				<ul>
				  <li><a href="https://arxiv.org/pdf/1607.01979v2.pdf">PDF</a></li>
				  <li><a href="https://github.com/gurkirt/actNet-inAct">Code</a></li>
				  <!-- <li><a href="http://arxiv.org/abs/1506.02078">Supplymentry</a></li> -->
				</ul>
            </div>
          </div>
        </div>
      </div>
    </div>
	
	<!--div class="pubwrap">
      <div class="row">
        <div class="col-md-6">
          <div class="pubimg">
            <img src="./resources/projects/continousGesture.png">
          </div>
        </div>
        <div class="col-md-6">
          <div class="pub">
            <div class="pubt">Continuous gesture recognition from articulated poses</div>
            <div class="pubd">This paper addresses the problem of continuous gesture recognition from articulated poses.
						Unlike the common isolated recognition scenario, the gesture boundaries are here unknown,
						and one has to solve two problems: segmentation and recognition.
						This is cast into a labeling framework, namely every site (frame) must be assigned a label (gesture ID).
						The inherent constraint for a piece-wise constant labeling is satisfied by solving a
						global optimization problem with a smoothness term.
						This mehtod secured 7th place in gesture
						detection task in ChaLearn LaP Challenge using only skeleton data.</div>
            <div class="puba">Georgios Evangelidis, Gurkirt Singh, Radu Patrice Horaud.</div>
			<div class="pubv">ECCV 2014 workshop</div>
            <div class="publ">
				<ul>
				  <li><a href="https://hal.archives-ouvertes.fr/hal-01082981/document">PDF</a></li>
				  <li><a href="https://team.inria.fr/perception/research/skeletalquads/">Project Page</a></li>
				  <li><a href="https://www.youtube.com/watch?v=d6wCy0CbYiE">Demo Video</a></li>
				</ul>
            </div>
          </div>
        </div>
      </div>
    </div>
	
	<div class="pubwrap">
      <div class="row">
        <div class="col-md-6">
          <div class="pubimg">
            <img src="./resources/projects/skeletalQuads.png">
          </div>
        </div>
        <div class="col-md-6">
          <div class="pub">
            <div class="pubt">Skeletal Quads:Human action recognition using joint quadruples</div>
            <div class="pubd"> In this context, we propose a local skeleton descriptor that encodes
								the relative position of joint quadruples. Such a coding implies a
								similarity normalisation transform that leads to a compact (6D or 5D)
								view-invariant skeletal feature, referred to as skeletal quad.
								In the references below, we use this descriptor in conjunction
								with FIsher kernel in order to encode gesture or action (sub)sequences.
								The short length of the descriptor compensates for the large inherent
								dimensionality associated to Fisher vectors.</div>
            <div class="puba">Georgios Evangelidis, Gurkirt Singh, Radu Patrice Horaud.</div>
			<div class="pubv">ICPR 2014</div>
            <div class="publ">
				<ul>
				  <li><a href="https://hal.archives-ouvertes.fr/file/index/docid/1064662/filename/main.pdf">PDF</a></li>
				  <li><a href="https://team.inria.fr/perception/research/skeletalquads/">Project Page</a></li>
				  <li><a href="https://team.inria.fr/perception/files/2015/05/SkeletalQuadCode.zip">Code</a></li> 
				</ul>
            </div>
          </div>
        </div>
      </div>
    </div>
	
	<div class="pubwrap">
      <div class="row">
        <div class="col-md-6">
          <div class="pubimg">
            <img src="./resources/projects/masterThesis.png">
          </div>
        </div>
        <div class="col-md-6">
          <div class="pub">
            <div class="pubt">Frame-wise representations of depth videos for action recognition</div>
            <div class="pubd"> We present three types of depth data representation from depth frames, which are referred as single-reference representation, multiple-reference representation and Quad representation.</div>
            <div class="puba">Gurkirt Singh</div>
			<div class="pubv">Master thesis, INRIA and Grenoble Institute of Technology, France, 2013</div>
			<div class="puba">Supervisors: Dr. Radu Horaud and Dr. Georgios Evangelidis</div>
            <div class="publ">
				<ul>
				  <li><a href="./resources/projects/masterReport.pdf">PDF</a></li>
				  <li><a href="https://sites.google.com/site/gurkirtcv/Research/master-thesis">Project Page</a></li>
				</ul>
            </div>
          </div>
        </div>
      </div>
    </div>
	
	<div class="pubwrap">
      <div class="row">
        <div class="col-md-6">
          <div class="pubimg">
            <img src="./resources/projects/btechThesis.png">
          </div>
        </div>
        <div class="col-md-6">
          <div class="pub">
            <div class="pubt">Categorizing Abnormal behavior from and indoor overhead camera</div>
            <div class="pubd"> We propose an approach using overhead camera to detect abnormal activties with help trajectory classification.</div>
            <div class="puba">Gurkirt Singh</div>
			<div class="pubv">Bachelor thesis, University of Edinburgh and VIT University, 2010</div>
			<div class="puba">Supervisor: Dr. Bob Fisher</div>
            <div class="publ">
				<ul>
				  <li><a href="http://homepages.inf.ed.ac.uk/rbf/FORUMTRACKING/SINGH/GurkirtSingh.pdf">PDF</a></li>
				  <li><a href="http://homepages.inf.ed.ac.uk/rbf/FORUMTRACKING/">Database and Project</a></li>
				  
				</ul>
            </div>
          </div>
        </div>
      </div>
    </div>
	
	
	</div>
	-->
	

	
	<!--div class="container">
		<h2 class="hh">MISC</h2>
		<p> I made an attempt to compile recent works on action recognition in more searchable format.
		Check it out on my <a href="https://sites.google.com/site/gurkirtcv/Research/action-recognition-review">older page</a></p>
		<p> My old research <a href="https://sites.google.com/site/gurkirtcv/Research">page</a>, it has an intresting review of action recognition and prediction works.</p>
		<p><a href="read/index.html">Citation Graph: part of submission from reading Group-1 at ICVSS 2016 (only works in firefox)</a></p>
		<div style="margin-bottom:30px"></div>
    <a href="https://info.flagcounter.com/c9RO"><img src="https://s11.flagcounter.com/count2/c9RO/bg_FFFFFF/txt_000000/border_CCCCCC/columns_5/maxflags_10/viewers_0/labels_0/pageviews_0/flags_0/percent_0/" alt="Flag Counter" border="0"></a>
  </div>
	-->
	<div class="container">
		
		<a style="margin-bottom:400px" href="#"> ==================================================  </a></div>
	<div style="margin-bottom:30px"></div>
	</div>
	
	<div class="container">
	<!-- Counter Code START 
	<a href="http://www.e-zeeinternet.com/" target="_blank"><img src="http://www.e-zeeinternet.com/count.php?page=1153724&style=default&nbdigits=5&reloads=1" alt="Free Counter" border="0" ></a>
	<a href="http://www.e-zeeinternet.com/" target="_blank"><img src="http://www.e-zeeinternet.com/count.php?page=1153725&style=default&nbdigits=5" alt="Free Web Counter" border="0" ></a>
	-->
	<div style="margin-bottom:30px"></div>
	</div>
	<div class="container"><footer> <p> 2021 copyright @ Sagar Chakraborty </p></div>
	<!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="./bootstrap/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
    <script src="./bootstrap/bootstrap.min.js"></script>
    <!-- IE10 viewport hack for Surface/ Windows 8 bug -->
    <script src="./bootstrap/ie10-viewport-bug-workaround.js"></script>
	
</body>
</html>
